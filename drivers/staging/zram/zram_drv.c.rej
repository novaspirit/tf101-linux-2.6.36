--- drivers/staging/zram/zram_drv.c
+++ drivers/staging/zram/zram_drv.c
@@ -195,82 +191,90 @@
 }
 
 static void handle_uncompressed_page(struct zram *zram,
-				struct page *page, u32 index)
+				struct page *bio_page, u32 index)
 {
-	unsigned char *user_mem, *cmem;
+	u32 zoffset;
+	struct page *zpage;
+	unsigned char *bio_mem, *zmem;
 
-	user_mem = kmap_atomic(page, KM_USER0);
-	cmem = kmap_atomic(zram->table[index].page, KM_USER1) +
-			zram->table[index].offset;
+	zram_find_obj(zram, index, &zpage, &zoffset);
+	BUG_ON(zoffset);
 
-	memcpy(user_mem, cmem, PAGE_SIZE);
-	kunmap_atomic(user_mem, KM_USER0);
-	kunmap_atomic(cmem, KM_USER1);
+	bio_mem = kmap_atomic(bio_page, KM_USER0);
+	zmem = kmap_atomic(zpage, KM_USER1);
 
-	flush_dcache_page(page);
+	memcpy(bio_mem, zmem, PAGE_SIZE);
+	kunmap_atomic(bio_mem, KM_USER0);
+	kunmap_atomic(zmem, KM_USER1);
+
+	flush_dcache_page(bio_page);
 }
 
 static int zram_read(struct zram *zram, struct bio *bio)
 {
-
 	int i;
 	u32 index;
 	struct bio_vec *bvec;
 
-	zram_stat64_inc(zram, &zram->stats.num_reads);
+	if (unlikely(!zram->init_done)) {
+		set_bit(BIO_UPTODATE, &bio->bi_flags);
+		bio_endio(bio, 0);
+		return 0;
+	}
 
+	zram_inc_stat(zram, ZRAM_STAT_NUM_READS);
 	index = bio->bi_sector >> SECTORS_PER_PAGE_SHIFT;
+
 	bio_for_each_segment(bvec, bio, i) {
 		int ret;
-		size_t clen;
-		struct page *page;
-		struct zobj_header *zheader;
-		unsigned char *user_mem, *cmem;
+		size_t zlen;
+		u32 zoffset;
+		struct page *bio_page, *zpage;
+		unsigned char *bio_mem, *zmem;
 
-		page = bvec->bv_page;
+		bio_page = bvec->bv_page;
 
-		if (zram_test_flag(zram, index, ZRAM_ZERO)) {
-			handle_zero_page(page);
+		if (zram_is_zero_page(zram, index)) {
+			handle_zero_page(bio_page);
 			continue;
 		}
 
+		zram_find_obj(zram, index, &zpage, &zoffset);
+
 		/* Requested page is not present in compressed area */
-		if (unlikely(!zram->table[index].page)) {
-			pr_debug("Read before write: sector=%lu, size=%u",
+		if (unlikely(!zpage)) {
+			pr_debug("Read before write on swap device: "
+				"sector=%lu, size=%u",
 				(ulong)(bio->bi_sector), bio->bi_size);
 			/* Do nothing */
 			continue;
 		}
 
 		/* Page is stored uncompressed since it's incompressible */
-		if (unlikely(zram_test_flag(zram, index, ZRAM_UNCOMPRESSED))) {
-			handle_uncompressed_page(zram, page, index);
+		if (unlikely(!zoffset)) {
+			handle_uncompressed_page(zram, bio_page, index);
 			continue;
 		}
 
-		user_mem = kmap_atomic(page, KM_USER0);
-		clen = PAGE_SIZE;
+		bio_mem = kmap_atomic(bio_page, KM_USER0);
+		zlen = PAGE_SIZE;
 
-		cmem = kmap_atomic(zram->table[index].page, KM_USER1) +
-				zram->table[index].offset;
+		zmem = kmap_atomic(zpage, KM_USER1) + zoffset;
 
-		ret = lzo1x_decompress_safe(
-			cmem + sizeof(*zheader),
-			xv_get_object_size(cmem) - sizeof(*zheader),
-			user_mem, &clen);
+		ret = lzo1x_decompress_safe(zmem, xv_get_object_size(zmem),
+					bio_mem, &zlen);
 
-		kunmap_atomic(user_mem, KM_USER0);
-		kunmap_atomic(cmem, KM_USER1);
+		kunmap_atomic(bio_mem, KM_USER0);
+		kunmap_atomic(zmem, KM_USER1);
 
-		/* Should NEVER happen. Return bio error if it does. */
+		/* This should NEVER happen - return bio error if it does! */
 		if (unlikely(ret != LZO_E_OK)) {
 			pr_err("Decompression failed! err=%d, page=%u\n",
 				ret, index);
-			zram_stat64_inc(zram, &zram->stats.failed_reads);
 			goto out;
 		}
 
-		flush_dcache_page(page);
+		flush_dcache_page(bio_page);
 		index++;
 	}
 
@@ -285,119 +289,103 @@
 
 static int zram_write(struct zram *zram, struct bio *bio)
 {
-	int i;
+	int i, ret;
 	u32 index;
 	struct bio_vec *bvec;
 
-	zram_stat64_inc(zram, &zram->stats.num_writes);
+	if (unlikely(!zram->init_done)) {
+		ret = zram_init_device(zram);
+		if (ret)
+			goto out;
+	}
 
+	zram_inc_stat(zram, ZRAM_STAT_NUM_WRITES);
 	index = bio->bi_sector >> SECTORS_PER_PAGE_SHIFT;
 
 	bio_for_each_segment(bvec, bio, i) {
-		int ret;
-		u32 offset;
-		size_t clen;
-		struct zobj_header *zheader;
-		struct page *page, *page_store;
-		unsigned char *user_mem, *cmem, *src;
+		size_t zlen;
+		u32 zoffset;
+		struct page *bio_page, *zpage;
+		unsigned char *zbuffer, *zworkmem;
+		unsigned char *bio_mem, *zmem, *src;
 
-		page = bvec->bv_page;
-		src = zram->compress_buffer;
+		bio_page = bvec->bv_page;
 
 		/*
 		 * System overwrites unused sectors. Free memory associated
-		 * with this sector now.
+		 * with this sector now (if used).
 		 */
-		if (zram->table[index].page ||
-				zram_test_flag(zram, index, ZRAM_ZERO))
-			zram_free_page(zram, index);
-
-		mutex_lock(&zram->lock);
-
-		user_mem = kmap_atomic(page, KM_USER0);
-		if (page_zero_filled(user_mem)) {
-			kunmap_atomic(user_mem, KM_USER0);
-			mutex_unlock(&zram->lock);
-			zram_stat_inc(&zram->stats.pages_zero);
-			zram_set_flag(zram, index, ZRAM_ZERO);
+		zram_free_page(zram, index);
+
+		preempt_disable();
+		zbuffer = __get_cpu_var(compress_buffer);
+		zworkmem = __get_cpu_var(compress_workmem);
+		if (unlikely(!zbuffer || !zworkmem)) {
+			preempt_enable();
+			goto out;
+		}
+
+		src = zbuffer;
+		bio_mem = kmap_atomic(bio_page, KM_USER0);
+		if (page_zero_filled(bio_mem)) {
+			kunmap_atomic(bio_mem, KM_USER0);
+			preempt_enable();
+			zram_inc_stat(zram, ZRAM_STAT_PAGES_ZERO);
+			zram_set_zero_page(zram, index);
 			continue;
 		}
 
-		ret = lzo1x_1_compress(user_mem, PAGE_SIZE, src, &clen,
-					zram->compress_workmem);
+		ret = lzo1x_1_compress(bio_mem, PAGE_SIZE, src, &zlen,
+					zworkmem);
 
-		kunmap_atomic(user_mem, KM_USER0);
+		kunmap_atomic(bio_mem, KM_USER0);
 
 		if (unlikely(ret != LZO_E_OK)) {
-			mutex_unlock(&zram->lock);
+			preempt_enable();
 			pr_err("Compression failed! err=%d\n", ret);
-			zram_stat64_inc(zram, &zram->stats.failed_writes);
 			goto out;
 		}
 
-		/*
-		 * Page is incompressible. Store it as-is (uncompressed)
-		 * since we do not want to return too many disk write
-		 * errors which has side effect of hanging the system.
-		 */
-		if (unlikely(clen > max_zpage_size)) {
-			clen = PAGE_SIZE;
-			page_store = alloc_page(GFP_NOIO | __GFP_HIGHMEM);
-			if (unlikely(!page_store)) {
-				mutex_unlock(&zram->lock);
+		 /* Page is incompressible. Store it as-is (uncompressed) */
+		if (unlikely(zlen > max_zpage_size)) {
+			zlen = PAGE_SIZE;
+			zpage = alloc_page(GFP_NOWAIT | __GFP_HIGHMEM);
+			if (unlikely(!zpage)) {
+				preempt_enable();
 				pr_info("Error allocating memory for "
 					"incompressible page: %u\n", index);
-				zram_stat64_inc(zram,
-					&zram->stats.failed_writes);
 				goto out;
 			}
 
-			offset = 0;
-			zram_set_flag(zram, index, ZRAM_UNCOMPRESSED);
-			zram_stat_inc(&zram->stats.pages_expand);
-			zram->table[index].page = page_store;
-			src = kmap_atomic(page, KM_USER0);
+			zoffset = 0;
+			zram_inc_stat(zram, ZRAM_STAT_PAGES_EXPAND);
+			src = kmap_atomic(zpage, KM_USER0);
 			goto memstore;
 		}
 
-		if (xv_malloc(zram->mem_pool, clen + sizeof(*zheader),
-				&zram->table[index].page, &offset,
-				GFP_NOIO | __GFP_HIGHMEM)) {
-			mutex_unlock(&zram->lock);
+		if (xv_malloc(zram->mem_pool, zlen, &zpage, &zoffset,
+				GFP_NOWAIT | __GFP_HIGHMEM)) {
+			preempt_enable();
 			pr_info("Error allocating memory for compressed "
-				"page: %u, size=%zu\n", index, clen);
-			zram_stat64_inc(zram, &zram->stats.failed_writes);
+				"page: %u, size=%zu\n", index, zlen);
 			goto out;
 		}
 
 memstore:
-		zram->table[index].offset = offset;
-
-		cmem = kmap_atomic(zram->table[index].page, KM_USER1) +
-				zram->table[index].offset;
-
-#if 0
-		/* Back-reference needed for memory defragmentation */
-		if (!zram_test_flag(zram, index, ZRAM_UNCOMPRESSED)) {
-			zheader = (struct zobj_header *)cmem;
-			zheader->table_idx = index;
-			cmem += sizeof(*zheader);
-		}
-#endif
+		zmem = kmap_atomic(zpage, KM_USER1) + zoffset;
 
-		memcpy(cmem, src, clen);
+		memcpy(zmem, src, zlen);
+		kunmap_atomic(zmem, KM_USER1);
+		preempt_enable();
 
-		kunmap_atomic(cmem, KM_USER1);
-		if (unlikely(zram_test_flag(zram, index, ZRAM_UNCOMPRESSED)))
+		if (unlikely(!zoffset))
 			kunmap_atomic(src, KM_USER0);
 
 		/* Update stats */
-		zram->stats.compr_size += clen;
-		zram_stat_inc(&zram->stats.pages_stored);
-		if (clen <= PAGE_SIZE / 2)
-			zram_stat_inc(&zram->stats.good_compress);
+		zram_add_stat(zram, ZRAM_STAT_COMPR_SIZE, zlen);
+		zram_inc_stat(zram, ZRAM_STAT_PAGES_STORED);
 
-		mutex_unlock(&zram->lock);
+		zram_insert_obj(zram, index, zpage, zoffset);
 		index++;
 	}
 
